# -*- coding: utf-8 -*-
"""Sentiment Analysis GC.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cP8ykrM-LGfqo1xZhw0kfPxBZTGfpD-1
"""

#load the data

import pandas as pd
file_path = '/content/yelp_labelled.txt'

# Read the text file
with open(file_path, 'r', encoding='utf-8') as file:
    reviews = file.readlines()

# Create a DataFrame from the reviews
yelp_df = pd.DataFrame(reviews, columns=['review'])

pd.set_option('display.max_columns', None)

yelp_df.head(10)

#presence of unusual characters

import re

# Function to find unusual characters
def find_unusual_characters(text):
    unusual_chars = re.findall(r'[^\w\s,]', text)  # Find characters that are not alphanumeric, whitespace, or comma
    return unusual_chars

# Apply the function to the reviews
yelp_df['unusual_characters'] = yelp_df['review'].apply(find_unusual_characters)

# Count reviews with unusual characters
unusual_char_count = yelp_df['unusual_characters'].apply(lambda x: len(x)).sum()

#see all rows in the output
#pd.set_option('display.max_rows', None)

print(f'Total unusual characters found: {unusual_char_count}')
print(f'Sample unusual characters: {yelp_df["unusual_characters"]}')

#vocabulary size
import nltk
nltk.download('punkt')
from collections import Counter
from nltk.tokenize import word_tokenize

# Tokenize reviews
yelp_df['tokens'] = yelp_df['review'].apply(word_tokenize)

# Flatten the list of tokens and count unique words
all_tokens = [token for sublist in yelp_df['tokens'] for token in sublist]
vocabulary_size = len(set(all_tokens))

print(f'Vocabulary size: {vocabulary_size}')

#proposed word embedding length

# Example choice of word embedding length
embedding_length = 100
print(f'Proposed word embedding length: {embedding_length}')

#statistical justification for chosen max sequence length

import matplotlib.pyplot as plt

# Calculate review lengths
yelp_df['review_length'] = yelp_df['tokens'].apply(len)

# Plot the distribution of review lengths
plt.hist(yelp_df['review_length'], bins=50)
plt.xlabel('Review Length')
plt.ylabel('Frequency')
plt.title('Distribution of Review Lengths')
plt.show()

# Choose a max sequence length based on the distribution (e.g., 95th percentile)
max_seq_length = int(yelp_df['review_length'].quantile(0.95))
print(f'Chosen maximum sequence length: {max_seq_length}')

#clean and label the data

import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import re

# Download the necessary NLTK resources
nltk.download('stopwords')
nltk.download('wordnet')

# Define the cleaning function
def clean_text(text):
    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags
    text = re.sub(r'[^a-zA-Z\s]', '', text, re.I | re.A)  # Remove special characters
    text = text.lower()  # Lowercase text
    text = text.strip()  # Remove whitespace
    tokens = text.split()  # Tokenize text
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stopwords.words('english')]
    return ' '.join(tokens)

# Verify the column name and adjust accordingly
print(yelp_df.columns)

# Apply the cleaning function to your dataset using the correct column name
yelp_df['cleaned_text'] = yelp_df['review'].apply(clean_text)

# Ensure your dataset has a column label with sentiment categories: positive, neutral, negative
def assign_label(text):
    positive_words = ['love', 'great', 'fantastic', 'excellent']
    negative_words = ['hate', 'bad', 'terrible', 'awful']

    if any(word in text for word in positive_words):
        return 'positive'
    elif any(word in text for word in negative_words):
        return 'negative'
    else:
        return 'neutral'

# Apply the function to create a new 'label' column
yelp_df['label'] = yelp_df['cleaned_text'].apply(assign_label)

# Create a new DataFrame for the cleaned and labeled data
cleaned_yelp_df = yelp_df[['review', 'cleaned_text', 'label']]

# Print the first few rows to verify
print(cleaned_yelp_df.head())

#normalize text during the tokenization process

import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import string

# Ensure necessary resources are downloaded
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')



# Function to tokenize and normalize text
def tokenize_normalize(text):
    # Tokenization
    tokens = word_tokenize(text)

    # Normalization
    tokens = [token.lower() for token in tokens]
    tokens = [token for token in tokens if token not in string.punctuation]
    stop_words = set(stopwords.words('english'))
    tokens = [token for token in tokens if token not in stop_words]
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(token) for token in tokens]

    return tokens

# Apply tokenization and normalization to the 'cleaned_text' column
cleaned_yelp_df['normalized_tokens'] = cleaned_yelp_df['cleaned_text'].apply(tokenize_normalize)

# Print the DataFrame to verify the results
print(cleaned_yelp_df[['cleaned_text', 'normalized_tokens']])

#text vectorization

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences


# Initialize Tokenizer
tokenizer = Tokenizer(num_words=5000)  # Limit vocabulary size to 5000 most frequent words
tokenizer.fit_on_texts(cleaned_yelp_df['cleaned_text'])

# Convert text to sequences of integers
sequences = tokenizer.texts_to_sequences(cleaned_yelp_df['cleaned_text'])

# Vocabulary index
word_index = tokenizer.word_index

# Pad sequences to ensure uniform length (assuming max length is 26)
maxlen = 26
data_padded = pad_sequences(sequences, maxlen=maxlen)

# Example of accessing the first padded sequence
print(f"Example of padded sequence:\n{data_padded[0]}")

# Example of accessing word index
print(f"\nExample of word index:\n{word_index}")

#pre padding

import numpy as np
import pandas as pd
from tensorflow.keras.preprocessing.sequence import pad_sequences


# Collect all unique tokens from the 'normalized_tokens' column
all_tokens = [token for tokens_list in cleaned_yelp_df['normalized_tokens'] for token in tokens_list]
unique_tokens = list(set(all_tokens))

# Create a token to index mapping
token_to_index = {token: idx + 1 for idx, token in enumerate(unique_tokens)}  # Start index from 1
token_to_index['<PAD>'] = 0  # Add padding token

# Function to convert tokens to indices
def tokens_to_indices(tokens_list, token_to_index):
    return [[token_to_index[token] for token in tokens] for tokens in tokens_list]

# Convert tokens in 'normalized_tokens' to indices
cleaned_yelp_df['token_indices'] = tokens_to_indices(cleaned_yelp_df['normalized_tokens'], token_to_index)

# Function to pad sequences with pre-padding
def pad_sequences_pre(sequences, max_length, padding_value=0):
    padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='pre', truncating='post', value=padding_value)
    return padded_sequences.tolist()  # Convert to list of lists

# Pad sequences in 'token_indices' column with pre-padding
max_length = 26
cleaned_yelp_df['padded_tokens'] = pad_sequences_pre(cleaned_yelp_df['token_indices'], max_length, padding_value=token_to_index['<PAD>'])

# Print original and padded sequences
print("Original sequences and padded sequences:")
for idx, row in cleaned_yelp_df.iterrows():
    print(f"Review: {row['cleaned_text']}")
    print(f"Original tokens: {row['normalized_tokens']}")
    print(f"Token indices: {row['token_indices']}")
    print(f"Padded tokens: {row['padded_tokens']}")
    print()

# Print a single padded sequence for screenshot
single_review_index = 0  # Change index to get a different review
review = cleaned_yelp_df.loc[single_review_index, 'cleaned_text']
original_tokens = cleaned_yelp_df.loc[single_review_index, 'normalized_tokens']
token_indices = cleaned_yelp_df.loc[single_review_index, 'token_indices']
padded_tokens = cleaned_yelp_df.loc[single_review_index, 'padded_tokens']

print(f"Review: {review}")
print(f"Original tokens: {original_tokens}")
print(f"Token indices: {token_indices}")
print(f"Padded tokens: {padded_tokens}")

# split the dataset properly - train, test, validation
#Identify how many categories of sentiment will be used and an activation function for the final dense layer of the network.


from sklearn.model_selection import train_test_split
from keras.models import Sequential
from keras.layers import Embedding, LSTM, Dense, Dropout
from keras.callbacks import EarlyStopping
import numpy as np
import pandas as pd
import tensorflow as tf
import random

# Set random seeds for reproducibility
seed = 42
tf.random.set_seed(seed)
np.random.seed(seed)
random.seed(seed)


# Split data into training and temporary (remaining) data
X_train_temp, X_test, y_train_temp, y_test = train_test_split(
    cleaned_yelp_df['padded_tokens'], cleaned_yelp_df['label'], test_size=0.10, random_state=42, stratify=cleaned_yelp_df['label']
)

# Further split temporary data into validation and final training data
X_train, X_val, y_train, y_val = train_test_split(
    X_train_temp, y_train_temp, test_size=0.1111, random_state=42, stratify=y_train_temp
)

# Print sizes of each set
print(f"Training data size: {len(X_train)}")
print(f"Validation data size: {len(X_val)}")
print(f"Testing data size: {len(X_test)}")

# Convert data to numpy arrays and one-hot encode labels
X_train = np.array(list(X_train))
X_val = np.array(list(X_val))
X_test = np.array(list(X_test))
y_train = np.array(pd.get_dummies(y_train))
y_val = np.array(pd.get_dummies(y_val))
y_test = np.array(pd.get_dummies(y_test))

# Define the number of output classes
num_classes = y_train.shape[1]  # Number of unique labels

# Initialize the model
model = Sequential()

# Add an embedding layer
input_dim = 2354  # vocabulary size
output_dim = 100  # embedding size
input_length = 26  # Example input length (number of words per input)
model.add(Embedding(input_dim=input_dim, output_dim=output_dim, input_length=input_length))

# Add an LSTM layer
model.add(LSTM(128, return_sequences=True))
model.add(Dropout(0.2))  # Adding dropout for regularization
model.add(LSTM(64))

# Add a fully connected layer
model.add(Dense(32, activation='relu'))
model.add(Dropout(0.2))

# Add the final dense layer with softmax activation
model.add(Dense(num_classes, activation='softmax'))

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Print the model summary
model.summary()

# Print details about sentiment categories and activation function
print(f"Number of sentiment categories: {num_classes}")
print(f"Activation function for the final dense layer: softmax")

# Define EarlyStopping callback with patience
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

# Train the model with early stopping
history = model.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val), callbacks=[early_stopping])

# Print the stopping criteria and final accuracy
print(f"Stopped after {len(history.epoch)} epochs due to early stopping.")
print(f"Best validation accuracy: {max(history.history['val_accuracy'])}")

#viz of model training process
#line graph of loss and accuracy

import matplotlib.pyplot as plt

# Plot training & validation loss values
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')

# Plot training & validation accuracy values (or another metric)
plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')

plt.show()

#save the trained network within the neural network

from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

# Define early stopping and model checkpoint callbacks
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
model_checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss')

# Train the model with the callbacks
history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=100,
                    callbacks=[early_stopping, model_checkpoint])

# Save the final model (optional)
model.save('final_model.h5')

# Export copy of prepared dataset

from google.colab import drive
import pandas as pd

# Mount Google Drive
drive.mount('/content/drive')

# Create a DataFrame with training data
train_data = pd.DataFrame({'tokens': list(X_train), 'label': list(y_train)})

# Define the file path in your Google Drive
file_path = '/content/drive/My Drive/prepared_training_data_1.csv'

# Save the prepared training dataset to a CSV file in Google Drive
train_data.to_csv(file_path, index=False)

print(f"Training data saved to {file_path}")